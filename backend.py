# backend.py â€” OFFLINE RAG + Qwen0.5B (NO ACCELERATE VERSION)

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import faiss
import numpy as np
import torch
from typing import List, Dict
import os
import re

# ===========================================
# FASTAPI
# ===========================================
app = FastAPI(title="iPhone Chatbot â€“ Offline RAG No-Hallucination")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

print("ğŸš€ STARTING OFFLINE RAG SYSTEM...\n")

# ===========================================
# LOAD EMBEDDING MODEL
# ===========================================
print("[1/5] Loading embedding model...")
embedding_model = SentenceTransformer("./models/embedding_model")
print("âœ… Embedding model loaded!\n")

# ===========================================
# LOAD QWEN 0.5B â€” KHÃ”NG DÃ™NG device_map!
# ===========================================
print("[2/5] Loading Qwen2.5-0.5B-Instruct...")

LLM_MODEL_PATH = "./models/llm_model"

tokenizer = AutoTokenizer.from_pretrained(
    LLM_MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True
)

llm_model = AutoModelForCausalLM.from_pretrained(
    LLM_MODEL_PATH,
    trust_remote_code=True,
    dtype=torch.float32,
    low_cpu_mem_usage=True,
    local_files_only=True
)

# CHUYá»‚N MODEL SANG CPU (KHÃ”NG cáº§n accelerate)
llm_model.to("cpu")
llm_model.eval()
print("âœ… Qwen 0.5B loaded!\n")

# ===========================================
# LOAD phones.txt
# ===========================================
print("[3/5] Loading phones.txt...")

def load_phone_data(path: str) -> List[Dict]:
    with open(path, "r", encoding="utf-8") as f:
        content = f.read()

    blocks = content.strip().split("\n\n")
    products = []

    for block in blocks:
        lines = block.strip().split("\n")
        if not lines:
            continue

        p = {"name": lines[0], "full": block}
        for line in lines[1:]:
            if ":" in line:
                k, v = line.split(":", 1)
                p[k.strip().lower()] = v.strip()

        products.append(p)

    return products

phones = load_phone_data("phones.txt")
print(f"âœ… Loaded {len(phones)} products!\n")

# ===========================================
# LOAD FAISS INDEX
# ===========================================
print("[4/5] Loading FAISS index...")
if not os.path.exists("phones.index"):
    raise FileNotFoundError("phones.index not found. Run build_index.py first.")
faiss_index = faiss.read_index("phones.index")
print("âœ… FAISS index loaded!\n")

# ===========================================
# PRECOMPUTE EMBEDDINGS
# ===========================================
print("[5/5] Precomputing embeddings...")

def structured(p: Dict) -> str:
    keys = ["giÃ¡", "mÃ n hÃ¬nh", "chip", "camera", "pin"]
    return p["name"] + " | " + " | ".join([p[k] for k in keys if k in p])

product_texts = [structured(p) for p in phones]
product_embs = embedding_model.encode(product_texts, convert_to_numpy=True).astype("float32")
product_norms = np.linalg.norm(product_embs, axis=1)

print("âœ… Embedding ready!\n")

THRESHOLD = 0.20

# ===========================================
# RETRIEVE
# ===========================================
def retrieve(q: str) -> List[Dict]:
    q_emb = embedding_model.encode([q], convert_to_numpy=True)[0]
    q_norm = np.linalg.norm(q_emb)

    dist, idx = faiss_index.search(np.expand_dims(q_emb, 0), 5)

    results = []
    for i, d in zip(idx[0], dist[0]):
        p = phones[i].copy()
        # TÃ­nh Cosine Similarity
        cos = float(np.dot(q_emb, product_embs[i]) / (q_norm * product_norms[i] + 1e-9))
        p["distance"] = float(d)
        p["similarity"] = cos
        results.append(p)

    return sorted(results, key=lambda x: x["distance"])

# ===========================================
# LLM ANSWER
# ===========================================
def llm_answer(query: str, products: List[Dict]) -> str:
    top = products[0]
    
    # Kiá»ƒm tra ngá»¯ cáº£nh cÃ³ liÃªn quan Ä‘á»§ khÃ´ng
    if top["similarity"] < THRESHOLD:
        return "Em xin lá»—i, em chÆ°a tÃ¬m tháº¥y thÃ´ng tin chi tiáº¿t nÃ y trong dá»¯ liá»‡u sáº£n pháº©m."

    ctx = structured(top)

    # PROMPT NGHIÃŠM NGáº¶T (FIX Lá»–I TRáº¢ Lá»œI Láº C Äá»€/HALLUCINATION)
    prompt = f"""
Báº¡n lÃ  má»™t TRá»¢ LÃ TÆ¯ Váº¤N Sáº¢N PHáº¨M IPHONE CHUYÃŠN NGHIá»†P, lá»‹ch sá»±.
Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  CHá»ˆ TRáº¢ Lá»œI CÃ‚U Há»I cá»§a khÃ¡ch hÃ ng dá»±a trÃªn Dá»® LIá»†U Sáº¢N PHáº¨M Ä‘Æ°á»£c cung cáº¥p.

[Dá»® LIá»†U Sáº¢N PHáº¨M]
{ctx}

[CÃ‚U Há»I]
{query}

[HÆ¯á»šNG DáºªN Báº®T BUá»˜C]
1. KHÃ”NG Ä‘Æ°á»£c sá»­ dá»¥ng báº¥t ká»³ lá»i chÃ o, káº¿t thÃºc thÆ°, hoáº·c máº«u form nÃ o.
2. TUYá»†T Äá»I KHÃ”NG Bá»ŠA Äáº¶T hoáº·c TÆ¯ Váº¤N THÃ”NG TIN KHÃ”NG CÃ“ TRONG Dá»® LIá»†U.
3. Náº¿u khÃ´ng Ä‘á»§ thÃ´ng tin, CHá»ˆ tráº£ lá»i: "Em xin lá»—i, em chÆ°a tÃ¬m tháº¥y thÃ´ng tin chi tiáº¿t nÃ y trong dá»¯ liá»‡u sáº£n pháº©m."
4. Tráº£ lá»i ngáº¯n gá»n, tá»‘i Ä‘a 3 cÃ¢u.

TRáº¢ Lá»œI:
"""

    tks = tokenizer(prompt, return_tensors="pt").to("cpu")

    with torch.no_grad():
        out = llm_model.generate(
            **tks,
            max_new_tokens=150,
            temperature=0.0,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(out[0], skip_special_tokens=True)
    
    # POST-PROCESSING (FIX Lá»–I MáºªU FORM VÃ€ DÃ’NG RÃC)
    if "TRáº¢ Lá»œI:" in text:
        text = text.split("TRáº¢ Lá»œI:")[-1].strip()
    
    # Loáº¡i bá» cÃ¡c chuá»—i rÃ¡c/máº«u form (Hallucination)
    text = re.sub(r"\[.*?\]", "", text, flags=re.IGNORECASE) # Loáº¡i bá» báº¥t ká»³ chuá»—i nÃ o trong ngoáº·c []
    

    # 3. Loáº¡i bá» cÃ¡c dÃ²ng trá»‘ng hoáº·c rÃ¡c cÃ²n sÃ³t láº¡i
    lines = []
    for line in text.split('\n'):
        line_clean = line.strip()
        # Loáº¡i bá» cÃ¡c chuá»—i lá»—i Hallucination/Fallback phá»• biáº¿n
        if any(keyword in line_clean for keyword in [
            "Em xin lá»—i vÃ¬ sá»± nháº§m láº«n", 
            "Em cáº§n thÃªm thÃ´ng tin Ä‘á»ƒ giÃºp Ä‘á»¡", 
            "ChÃºc em thÃ nh cÃ´ng", 
            "TrÃ¢n trá»ng", 
            "Em xin lá»—i, tÃ´i khÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t vá»",
            "liÃªn há»‡ vá»›i tÃ´i",
            "email hotline",
            "info@iphone.com",
            "HÃ£y nhá»› ráº±ng",
            "0987654321"
        ]):
            continue # Bá» qua dÃ²ng chá»©a cÃ¡c lá»—i nÃ y
        
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº§u dÃ²ng khÃ´ng cáº§n thiáº¿t
        if line_clean:
            lines.append(line_clean)
            
    return '\n'.join(lines)

# ===========================================
# API
# ===========================================
class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str
    relevant_products: list

@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    q = req.message
    
    # KIá»‚M TRA NGOáº I Lá»† (FIX Lá»–I Há»I Sáº¢N PHáº¨M KHÃ”NG CÃ“)
    if any(brand in q.lower() for brand in ["samsung", "xiaomi", "oppo", "android"]):
        ans = "Em xin lá»—i, em chá»‰ cÃ³ dá»¯ liá»‡u vá» cÃ¡c dÃ²ng iPhone. Xin quÃ½ khÃ¡ch vui lÃ²ng há»i vá» sáº£n pháº©m iPhone."
        return ChatResponse(response=ans, relevant_products=[])

    results = retrieve(q)
    ans = llm_answer(q, results)
    
    # Xá»¬ LÃ Lá»–I KHÃ”NG TÃŒM THáº¤Y CONTEXT TRÆ¯á»šC KHI TRáº¢ Lá»œI
    if ans == "Em xin lá»—i, em chÆ°a tÃ¬m tháº¥y thÃ´ng tin chi tiáº¿t nÃ y trong dá»¯ liá»‡u sáº£n pháº©m.":
        # Náº¿u LLM tráº£ lá»i fallback, ta váº«n kiá»ƒm tra ngÆ°á»¡ng
        if results and results[0]["similarity"] < THRESHOLD:
            return ChatResponse(response=ans, relevant_products=[])

    return ChatResponse(response=ans, relevant_products=results[:2])

@app.get("/")
def root():
    return {"status": "running", "products_loaded": len(phones)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)